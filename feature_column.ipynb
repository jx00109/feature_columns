{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import feature_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 4.,  2.],\n",
      "       [ 5.,  4.],\n",
      "       [ 8.,  6.],\n",
      "       [-1.,  8.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "numeric_column(\n",
    "    key,\n",
    "    shape=(1,),\n",
    "    default_value=None,\n",
    "    dtype=tf.float32,\n",
    "    normalizer_fn=None\n",
    ")\n",
    "通过key对特征进行区别，产生能够将数值特征直接转化成tensor的feature_column\n",
    "对数值特征可以进行自定义的normalizer_fn映射\n",
    "\n",
    "对于default_value这个参数不甚理解\n",
    "\"\"\"\n",
    "samples = {\n",
    "    'price':[[1.], [2.], [3.], [4.]],\n",
    "    'other':[[6.], [7.], [10.], [1.]]\n",
    "}\n",
    "\n",
    "price_column = feature_column.numeric_column('price', normalizer_fn=lambda x: x*2)\n",
    "other_column = feature_column.numeric_column('other', default_value=-1., normalizer_fn=lambda x: x-2)\n",
    "\n",
    "columns=[price_column, other_column]\n",
    "tensor = feature_column.input_layer(samples, columns)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([tensor]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 4.,  2.,  1.,  0.,  0.,  0.],\n",
      "       [ 5.,  4.,  0.,  1.,  0.,  0.],\n",
      "       [ 8.,  6.,  0.,  0.,  1.,  0.],\n",
      "       [-1.,  8.,  0.,  0.,  0.,  1.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "tf.feature_column.bucketized_column(\n",
    "    source_column,\n",
    "    boundaries\n",
    ")\n",
    "分桶之后将维度从1升到了2\n",
    "\"\"\"\n",
    "samples['years'] = [1967, 1988, 1999, 2017]\n",
    "years_numeric_column = feature_column.numeric_column('years')\n",
    "years_bucketized_column = feature_column.bucketized_column(years_numeric_column, boundaries=[1980, 1990, 2000])\n",
    "\n",
    "columns = [price_column, other_column, years_bucketized_column]\n",
    "tensor = feature_column.input_layer(samples, columns)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([tensor]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 0., 1., 1., 0.],\n",
      "       [0., 0., 0., 2., 0.],\n",
      "       [1., 0., 1., 0., 0.],\n",
      "       [0., 2., 0., 0., 0.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "categorical_column_with_identity(\n",
    "    key,\n",
    "    num_buckets,\n",
    "    default_value=None\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "samples['pets']=[[2,3], [3, 3], [0, 2], [1, 1]]\n",
    "\n",
    "pets_column = feature_column.categorical_column_with_identity(key='pets', num_buckets=5)\n",
    "# indicator_column接受categorical_column作为参数，直接传categorical_column会报错\n",
    "# multi-hot representation of given categorical column\n",
    "pets_indicator_column = feature_column.indicator_column(pets_column)\n",
    "\n",
    "columns = [pets_indicator_column]\n",
    "tensor = feature_column.input_layer(samples, columns)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([tensor]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1., 1., 1., 1., 0., 1., 0.],\n",
      "       [1., 2., 1., 0., 0., 1., 0.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "categorical_column_with_vocabulary_list(\n",
    "    key,\n",
    "    vocabulary_list,\n",
    "    dtype=None,\n",
    "    default_value=-1,\n",
    "    num_oov_buckets=0\n",
    ")\n",
    "这个方法就是将一个单词列表生成为分类词汇特征列的\n",
    "对于oov，采用hash方式觉得位置\n",
    "\"\"\"\n",
    "samples['pets']=[['rabbit','pig','dog','mouse','cat'], ['rabbit','dog','dog','mouse','cat']]\n",
    "\n",
    "pets_vl_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key='pets',\n",
    "    vocabulary_list=['cat','dog','rabbit','pig'], \n",
    "    dtype=tf.string, \n",
    "    default_value=-1,\n",
    "    num_oov_buckets=3)\n",
    "\n",
    "indicator = tf.feature_column.indicator_column(pets_vl_column)\n",
    "tensor = tf.feature_column.input_layer(samples, [indicator])\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    print(session.run([tensor]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "categorical_column_with_hash_bucket(\n",
    "    key,\n",
    "    hash_bucket_size,\n",
    "    dtype=tf.string\n",
    ")\n",
    "仍然是分箱，但是这一次我们更加关心“我希望有多少分类？”，也许我们有150个单词，但我们只希望分成100个分类，多下来50个的怎么处理？\n",
    "取余数！101除以100余1，我们就把第101种单词也标记为1，和我们的第1种单词变成了同一类，如此类推，第102种和2种同属第2类,第103种和3种同属第3类...\n",
    "这看起来是错误的，不过很多时候tensorflow还是能够利用其他的特征列把它们区分开。所以，为了有效减少内存和计算时间，可以这么做。\n",
    "\"\"\"\n",
    "samples['colors'] = ['green','red','blue','yellow','pink','blue','red','indigo']\n",
    "\n",
    "colors_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        key='colors',\n",
    "        hash_bucket_size=5,\n",
    "    )\n",
    "\n",
    "indicator = tf.feature_column.indicator_column(colors_column)\n",
    "tensor = tf.feature_column.input_layer(samples, [indicator])\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    print(session.run([tensor]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=float32)]\n",
      "[array([[1., 0., 0.],\n",
      "       [0., 1., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 1., 0.]], dtype=float32)]\n",
      "[array([[0., 1., 0.],\n",
      "       [0., 1., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "samples['longtitude']=[19,61,30,9,45]\n",
    "samples['latitude']=[45,40,72,81,24]\n",
    "longtitude = tf.feature_column.numeric_column('longtitude')\n",
    "latitude = tf.feature_column.numeric_column('latitude')\n",
    "\n",
    "longtitude_b_c = tf.feature_column.bucketized_column(longtitude, [33,66])\n",
    "latitude_b_c  = tf.feature_column.bucketized_column(latitude,[33,66])\n",
    "\n",
    "# crossed_column不理解\n",
    "ll_column = tf.feature_column.crossed_column([longtitude_b_c, latitude_b_c], 12)\n",
    "\n",
    "indicator = tf.feature_column.indicator_column(ll_column)\n",
    "tensor = tf.feature_column.input_layer(samples, [indicator])\n",
    "only_longtitude = tf.feature_column.input_layer(samples, [longtitude_b_c])\n",
    "only_latitude = tf.feature_column.input_layer(samples, [latitude_b_c])\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    print(session.run([tensor]))\n",
    "    print(session.run([only_longtitude]))\n",
    "    print(session.run([only_latitude]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.00882747, -0.05484571, -0.13822883, -0.04347168,  0.00400347],\n",
      "       [ 0.22994633, -0.1435978 ,  0.18737102, -0.21302213,  0.25726706]],\n",
      "      dtype=float32)]\n",
      "[array([[1., 1., 1., 1.],\n",
      "       [1., 2., 1., 0.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "embedding_column(\n",
    "    categorical_column,\n",
    "    dimension,\n",
    "    combiner='mean',\n",
    "    initializer=None,\n",
    "    ckpt_to_load_from=None,\n",
    "    tensor_name_in_ckpt=None,\n",
    "    max_norm=None,\n",
    "    trainable=True\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "pets_f_c = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'pets',\n",
    "    ['cat','dog','rabbit','pig'], \n",
    "    dtype=tf.string, \n",
    "    default_value=-1)\n",
    "\n",
    "column = tf.feature_column.embedding_column(pets_f_c, 5)\n",
    "\n",
    "indicator = tf.feature_column.indicator_column(pets_f_c)\n",
    "\n",
    "tensor = tf.feature_column.input_layer(samples, [column])\n",
    "one_hot_tensor = tf.feature_column.input_layer(samples, [indicator])\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "\n",
    "    print(session.run([tensor]))\n",
    "    print(session.run([one_hot_tensor]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
